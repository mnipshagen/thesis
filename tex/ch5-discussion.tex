% !TeX root = ./main.tex

\section{Discussion}
\label{sec:discussion}
There are several topics to discuss for this thesis. First, there will be elaboration on the data itself, followed by a reflection on the methodology employed and results of this work. Lastly, possible applications and concerns will be discussed.

\subsection{Dataset}
While working with the dataset, it became apparent that it was created in a different field, with different priorities and a different kind of thinking. The structure of the data made it difficult to form coherent vectors and transform the data into a customary format. Some of the structural decision may have been decided for, since the data was continuously collected and the new interview data merged with the existing base. While the dataset was very wide in its coverage of topics, more details on academic achievements during the ongoing studies probably would have been helpful. For example, how many ECTS per subject area  or (sub-)module were earned instead of just a sum would have improved meaningfulness of the variable. Course or module specific grades or intermediate grades in general might have been a good indicator for performance as opposed to just having the grade of the last completed education available. The dataset was supplied in dta and sav format. Both are proprietary formats. The former belongs to Stata and the latter to SPSS, which are both closed-source, commercial software pieces. The data is complex and both of those suites integrate a lot of metadata, like alternative variable names, string variable values, etc. However, this can also be achieved with a custom binary format or text-based approaches, e.g. using a csv format for the raw data and then handle the metadata in xml/json or other structural file formats. However, pandas has a great import utility also supporting Stata and SPSS, which made data handling manageable. The documentation of the data also gives great insight into the structure of the data and where to find information one is looking for.

\subsection{Methodology}
Retrospectively, there are several things that could have gone better in surveying, preprocessing and analyzing the data.

The corrections made after the first results had been analyzed were more a patch than a proper redesign. In a similar or follow-up to this work, there should be a better survey included to determine variables, ill-defined for the problem. There seems to be rich data in the NEPS dataset, but it needs more manual attention and preparation than were feasible for this work. The large amount of qualitative data collected in NEPS is hard to extract from the digital systems of institutions, since they lack the introspection those questions provide. It could be analyzed on its own to identify qualitative measures that might be impactful on study success, but can also act as supporting data for quantitative data.

If the resources are available, the variables should be tagged with their measurement scale. This would remedy many of the complications faced during this work. Variables could be selectively onehot encoded, reducing the feature space for SVM and FFN classifiers. If there are real valued variables, they could be bucketed properly to improve the RFC and DTC classifier as well.

Some students had several spells of interest and hence appeared several times in the dataset. However, all samples were treated as if they were independently of each other. It might be sensible, and increase statistical validity, to handle a student who failed once and then succeeded, one who completed several programs or one who failed several times differently to a student that only contributed one sample.

If a larger dataset would be available, it might be advantageous to group the samples. More homogeneous groups will probably share similar struggles in their studies and come from more easily comparable backgrounds. This could improve prediction performance. Some examples of such groupings include:
\begin{enumerate*}[label=(\arabic*)]
    \item Grouping by field of study: The school math grade might be a better predicator for STEM subjects than liberal arts. Students of natural sciences and engineering are confronted with more higher math courses. Oppositely, predictions for liberal art students might improve by weighing their language grades higher.
    \item Grouping by institution: Students of universities or universities of applied science or colleges probably have different goals and strengths that indicates their success.
    \item Grouping by previous studies: Students that already changed subjects or completed another study program are at a different stage of education and personal development, than those entering higher education for the first time. As such, the difficulties they face again might differ in nature.
\end{enumerate*}
To train machine learning algorithms on these groups though, they need to be large enough. The NEPS dataset used in this thesis was too small to split and still expect reasonable results.

\subsection{Results}
\subsubsection{Training Process}
Even though the dataset was thoroughly examined and precautions had been taken, there were some unexpected results and overlooked flaws. The vocational training file should have been filtered more thoroughly as well as aggregated. In the 195 variables encompassed by the file, there were some inconspicuous indicators, that identified successful spells. When surveying those variables, they were only checked by keywords and manual inspection. Given a larger time frame, the dataset should be more comprehensibly surveyed and an aggregation for VocTrain formulated. A technique akin to the one employed to aggregate the other spell files could be used for aggregating VocTrain.

\subsubsection{Performance}
Even after the dataset was corrected three times, with a performance reduction each time, the performance of the classifiers reached around 85\% and 80\% when all data from VocTrain was additionally left out. Considering that the NEPS dataset had a small number of samples compared to other fiels of machine learning and also a comparibly small fraction of samples to feature space size, this result is still formidable. However, there is still room for improvement: There are more hyperparameters that could be tested, like a broader range on the SVM parameters, try gini against entropy in RFC and DTC, etc. With the changes proposed above, the existing parameter grid could be reevaluated as well. Given the smaller feature space when prepared according to measurement level, FFNs might perform on par as well. It might then be worth exploring different architectures of deep learning.

\subsubsection{Feature Importances}
Especially interesting is the analysis after removing the highly predictive variables from the training dataset. The variables in Correction-2 and Without VocTrain were increasingly selected from qualitative questions. "Subjective probability of obtaining a Master's degree", "Correspondence of study workload with study regulations?", "successful in studies compared to others" and "Motivation: Future career opportunities" are all variables whose answers rise from the student's introspection. A decrease in performance was to be expected after removing the highgly predicitve variables. However, the models still reach scores around 80\%, which is a satisfactory result. This result was achieved by combining quantitative variables, like how many ECTS points were achieved, whether a change of field occured or whether time abroad were incorporated, with qualitative variables. This implies that the student's introspective contributions might lead to a more accurate predictor for successfully finished degrees. Related work has mostly been considering quantitative data. In future studies, the student's personality traits and motivation could be well suited metrics to predict study success.

\subsection{Notes and Concerns}
This thesis was produced in the context of the SIDDATA project. Hence, the motivation was to gain insights into predicators of students' success so that areas in need of support can be identified. A study assistant that has access to such predicators could offer better tailored support to students or flag them for closer review by supportive staff (e.g. counseling, mentoring or the dean). However, data-driven prediction can be useful in different contexts. Several ideas have come up in literature and discussion with colleagues. An institution's funding as well as the evaluation of study programs is often dependent on the rate of drop outs. The so-called "retention rate" is also a measure of success and quality of a program. If students whose profiles suggest a higher chance of dropping out could be identified, they could be actively engaged and supported. This would, in the best case, lead to less stressed and more successful students, who can stay in the field they are interested in, which would also be advantageous for the institution. Taking this idea to the extreme, such predictions could be applied even earlier, in the admissions process. A common tool used for admission into undergraduate programs is the numerus clausus (NC). This is the averagse grade of the certificate for university entrance qualification, and is calculated in retrospect, after all admissions are done. The NC is the lowest average grade to still gain admission. A more refined predictor, similar to the one presented in this thesis, but also takes  more factors than the average grade into account, should in theory be a better measure of whom to admit to a program. This idea does not come without pitfalls though. In fact, all three of these scenarios -- a study assistant, student monitoring and admission -- bear ethical challenges\cite{Slade.2013, Aiken.2000, HighLevelExpertGrouponArtificialIntelligence.08.04.2019}.\\
For one there is a discussion to have on privacy. When data of students is collected, stored and analyzed, there need to be considerations for the storage location and interpretation of it. There must be informed consent, ensured privacy and de-identification of data. The aggregated data must never fall back on the students they were collected from. There needs to be a clear structure and organization for managing, classifying and storing the data \cite{Slade.2013}. The discussion about data privacy, usage and owner rights has seen a rise in popularity and sincerity over the last few years, especially with the General Data Protection Regulation (GDPR) of the European Union (EU)\footnote{\url{https://ec.europa.eu/commission/priorities/justice-and-fundamental-rights/data-protection/2018-reform-eu-data-protection-rules_en}}. The EU followed a similar pattern of thought, giving control of the data to the individual it is gathered from. It is a fine line to walk, especially since an assistant only works well with a history of personal data. It might be a sensible approach to keep all data of an individual separate of the aggregated data and only use it for predictions purposes. Once the student leaves the institution or opts out of the assistant program, the data can be de-identified and appended to the general database.\\
Furthermore, there are moral issues to consider. The discussion so far had the underlying assumption that higher education is only successful if and only if the student obtains a degree and finishes the program as planned. This definition of a "successful student" is by no means indisputable. Success could also be measured in personal development, and a change of field is no failure. Success as defined by the institution can then be outright orthogonal to success defined by the individual. Which rule to follow in such cases is difficult to discern. Would it be acceptable to nudge students into certain directions? If so, how much nudging and of which kind is deemed appropriate? How much influence do students have in the goal-setting of an assistant and the measure of success applied to them? Using AI in an educational setting needs to be handled carefully and with a "safety first" mindset. Questions such as these need answering before a system is implemented.